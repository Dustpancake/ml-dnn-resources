{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual weight extraction and NN calculations\n",
    "Weights in a NN are randomly initialized; as such, if a model fails to train, sometimes reinitalizing the weights and running the training steps again can remedy the issue.\n",
    "\n",
    "This is ubiquitous, but ineffective as an initialization. There has been much effort invested in determining how to optimally initialize weights, but an effective solution is to use Xavier-weight-algorithm ([Glorot & Bengio, 2006](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)). This algorithm still uses a PRNG, however uses a gaussian distribution over uniform centered on 0, with standard deviation related to the number of connections present in the current layer, such that\n",
    "$$\n",
    "\\sigma_i = \\sqrt{\\frac{2}{n_{\\text{in}, i} + n_{\\text{out}, i}}}\n",
    "$$\n",
    "gives the standard deviation for the $i^\\text{th}$ layer. Note, in this context *layer* refers to the *layer of weights*, not of neurons: there are $N-1$ weight layers for the typical $N$ layered NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow has [keyword arguments](https://keras.io/api/layers/initializers/) for specifying initializers. Available algorithms are in `tf.keras.initializers`:\n",
    "\n",
    "- `RandomNormal(mean=0.0, stddev=0.05, seed=None)`\n",
    "Normal distribution.\n",
    "- `RandomUniform(mean=0.0, stddev=0.05, seed=None)`\n",
    "Uniform distribution.\n",
    "- `TruncatedNormal(mean=0.0, stddev=0.05, seed=None)`\n",
    "Essentially the same as a normal distribution, however values more than $2 \\sigma$ from the mean are discarded and redrawn.\n",
    "- `Zeros()`\n",
    "All zero.\n",
    "- `Ones()`\n",
    "All ones.\n",
    "- `GlorotNormal(seed=None)`\n",
    "Xavier normal.\n",
    "- `GlorotUniform(seed=None)`\n",
    "Xavier uniform.\n",
    "- `Identity(gain=1.0)`\n",
    "Generates 2d identity matrix.\n",
    "- `Orthogonal(gain=1.0)`\n",
    "Orthogonal matrix.\n",
    "- `Constant(value=0)`\n",
    "Constant value.\n",
    "\n",
    "You can also create custom initializers with functions or classes (see the above link for documentation).\n",
    "\n",
    "\n",
    "These are given to a keras layer, e.g.\n",
    "```py\n",
    "initializer = tf.keras.initializers.RandomNormal()\n",
    "layer = tf.keras.layers.Dense(2, kernel_initializer=initializer)\n",
    "```\n",
    "Keras layers also have a `bias_initializer` keyword for using such an algorithm on the bias neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual weigth calculations\n",
    "For illustrative purposes, we will train a neural network on a logic gate. This is a bit like cracking a wallnut with a sledgehammer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR\n",
    "x = np.array([\n",
    "    [0,0], [1,0], [0,1], [1,1]\n",
    "])\n",
    "\n",
    "y = np.array(\n",
    "    [0, 1, 1, 0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(\n",
    "        2, \n",
    "        input_dim=x.shape[1],\n",
    "        activation='relu'\n",
    "    ),\n",
    "    tf.keras.layers.Dense(1) # output layer  \n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14c619f10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, verbose=0, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.6809308e-08]\n",
      " [9.9999988e-01]\n",
      " [9.9999994e-01]\n",
      " [2.9486669e-08]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* I had to dump the first trained version of the above model, ran the exact same code again, and optained a much better model -- the hazards of initialization are real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping weights\n",
    "We will now use `.layers.get_weights()` to extract the weights of each layer in the NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -- LAYER 0 ---------------- #\n",
      " - biases\n",
      "0B   \t->\tL1N0: 0.000\n",
      "0B   \t->\tL1N1: -0.000\n",
      " - weights\n",
      "L0N0 \t->\tL1N0: 1.276\n",
      "L0N0 \t->\tL1N1: -1.023\n",
      "L0N1 \t->\tL1N0: -1.276\n",
      "L0N1 \t->\tL1N1: 1.023\n",
      "\n",
      "# -- LAYER 1 ---------------- #\n",
      " - biases\n",
      "1B   \t->\tL2N0: 0.000\n",
      " - weights\n",
      "L1N0 \t->\tL2N0: 0.784\n",
      "L1N1 \t->\tL2N0: 0.977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_weights = []\n",
    "extracted_biases = []\n",
    "\n",
    "for i, layer in enumerate(model.layers):\n",
    "    weights = layer.get_weights()[0]\n",
    "    biases = layer.get_weights()[1]\n",
    "    print(f\"# -- LAYER {i} ---------------- #\")\n",
    "    \n",
    "    print(\" - biases\")\n",
    "    # j is the jth neuron in the ith layer\n",
    "    for j, bias in enumerate(biases):\n",
    "        print(f\"{i}B   \\t->\\tL{i+1}N{j}: {bias:.3f}\")\n",
    "        extracted_biases.append(bias)\n",
    "    \n",
    "    print(\" - weights\")    \n",
    "    for j_from, w1 in enumerate(weights):\n",
    "        for j_to, w2 in enumerate(w1):\n",
    "            print(f\"L{i}N{j_from} \\t->\\tL{i+1}N{j_to}: {w2:.3f}\")\n",
    "            extracted_weights.append(w2)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the extracted weights to manually calculate the output of the NN. The algorithm for this is\n",
    "$$\n",
    "S_k = \\left( \\sum_i n_i \\cdot w_i \\right) + b_k\n",
    "$$\n",
    "for the $k^\\text{th}$ neuron sum, followed by ReLU activation\n",
    "$$\n",
    "m_i = \\text{max}(0, S_i)\n",
    "$$\n",
    "giving the values of the $i^\\text{th}$ neuron in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden sum 0 = 1.276\n",
      "hidden sum 1 = -1.276\n",
      "hidden 0 = 1.276\n",
      "hidden 1 = 0.000\n",
      "output sum = 1.000\n",
      "-- ouput = 1.000\n"
     ]
    }
   ],
   "source": [
    "wgt = extracted_weights #Â alias to make typing easier\n",
    "bis = extracted_biases\n",
    "\n",
    "def manual_predict(x):\n",
    "    global wgt, bis\n",
    "    i_0, i_1 = x\n",
    "    \n",
    "    h_0_sum = (i_0 * wgt[0]) + (i_1 * wgt[1]) + (bis[0])\n",
    "    h_1_sum = (i_0 * wgt[2]) + (i_1 * wgt[3]) + (bis[1])\n",
    "    \n",
    "    print(f\"hidden sum 0 = {h_0_sum:.3f}\")\n",
    "    print(f\"hidden sum 1 = {h_1_sum:.3f}\")\n",
    "    \n",
    "    # relu activation\n",
    "    h_0 = max(0, h_0_sum)\n",
    "    h_1 = max(0, h_1_sum)\n",
    "    \n",
    "    print(f\"hidden 0 = {h_0:.3f}\")\n",
    "    print(f\"hidden 1 = {h_1:.3f}\")\n",
    "    \n",
    "    out_sum = (h_0 * wgt[4]) + (h_1 * wgt[5]) + (bis[2])\n",
    "    \n",
    "    print(f\"output sum = {out_sum:.3f}\")\n",
    "    \n",
    "    # relu activation\n",
    "    out_sum = max(0, out_sum)\n",
    "    \n",
    "    print(f\"-- ouput = {out_sum:.3f}\")\n",
    "\n",
    "manual_predict(x[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
