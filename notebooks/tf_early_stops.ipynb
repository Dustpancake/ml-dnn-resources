{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stops in NN\n",
    "A problem with complex or networks that train for too long, is that they may start to poorly perform on the validation set, and overfit to the training data.\n",
    "\n",
    "Often, for NN, we define three data sets\n",
    "- training\n",
    "- validation\n",
    "- holdout\n",
    "\n",
    "The holdout set is not seen by the NN until a final performance measure is needed, thus is supposed to represent an unbiased efficiency.\n",
    "\n",
    "For training and validation, normally an 80/20 split is used in favour of training data. We can use the validation to trigger early stops when the performance of the NN starts to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can seperate out our data sets with an `sklearn` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y\n",
    "    test_size=0.20,\n",
    "    random_state=414141  # seed value for prng\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Using the network defined in the Iris notebook, between `.compile()` and `.fit()` function calls, we will add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "monitor = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=1e-3,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='min'\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the [`EarlyStopping`](https://keras.io/api/callbacks/early_stopping/) class to monitor the loss value\n",
    "- `min_delta`: minimum change in error to be measured as an improvement. If `val_loss` changes by less than `min_delta`, we satisfy an early stop condition. \n",
    "- `patience`: how many epochs do we wait to stop training, provided the early_stop condition is met for each epoch.\n",
    "- `mode`: can be either `min`, `max`, or default to `auto` -- sets whether we want to monitor minimizing or maximizing changes.\n",
    "- `restore_best_weighs`: after stop is triggered, restores the weights where performance on the validation set is highest.\n",
    "\n",
    "In order to use this `monitor` instance, we adjust our `.fit()` function with a few new paraments, all of which are self-explanatory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_train, y_train, \n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[monitor],\n",
    "    epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evaluate our model exactly as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "We can use exactly the same prescription for regression models without any change to the function calls or parameters. Such is the power of abstraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
